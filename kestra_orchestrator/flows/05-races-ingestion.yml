id: 05-races-ingestion
namespace: zoomcamp-project
description: Ingest races data by year, scheduled to run every month on the first day of each month

inputs:
  - id: isBackfillAll
    type: BOOLEAN
    defaults: false

variables:
  date: "{{ trigger.date | date('yyyy') | default('2024') }}"
  start_year: "{{ inputs.start_year | date('yyyy') | default('2020') }}"
  end_year: "{{ inputs.end_year | date('yyyy') | default('2025') }}"

tasks:
  - id: conditional_branch
    type: io.kestra.core.tasks.flows.Switch
    value: "{{ inputs.isBackfillAll }}"
    cases:
        "true":
          - id: backfill_all_races_data
            type: io.kestra.plugin.scripts.python.Script
            runner: PROCESS
            env:
              DESTINATION__BIGQUERY__CREDENTIALS: "{{ kv('GCP_CREDS') }}"

            script: |
              import dlt
              import requests
              import datetime

              @dlt.resource(name="races", primary_key="raceId", write_disposition="replace")
              def races():
                base_url = "https://f1api.dev/api/"
                data_path = "races"

                for year in range({{ vars.start_year }}, {{ vars.end_year }}):
                  response = requests.get(f"{base_url}/{year}")
                  data = response.json()
                  if data_path in data and data[data_path]:
                        yield data[data_path]
                        
              pipeline = dlt.pipeline(
                  pipeline_name="f1_drivers_pipeline",
                  destination="bigquery",
                  dataset_name="f1",
                  dev_mode=False,
              )

              pipeline = dlt.pipeline(
                  pipeline_name="f1_drivers_pipeline",
                  destination="bigquery",
                  dataset_name="{{ kv('DATASET_NAME') }}",
                  dev_mode=False,
              )

              load_info = pipeline.run(races)
              print(load_info)

        "false":
          - id: ingest_races_data
            type: io.kestra.plugin.scripts.python.Script
            runner: PROCESS
            env:
              DESTINATION__BIGQUERY__CREDENTIALS: "{{ kv('GCP_CREDS') }}"

            script: |
              import dlt
              import requests
              import datetime

              @dlt.resource(name="races", primary_key="raceId", write_disposition="replace")
              def races():
                base_url = "https://f1api.dev/api/"
                data_path = "races"

                response = requests.get(f"{base_url}/{{render(vars.date)}}")
                data = response.json()
                if data_path in data and data[data_path]:
                      yield data[data_path]
                        
              pipeline = dlt.pipeline(
                  pipeline_name="f1_drivers_pipeline",
                  destination="bigquery",
                  dataset_name="f1",
                  dev_mode=False,
              )

              pipeline = dlt.pipeline(
                  pipeline_name="f1_drivers_pipeline",
                  destination="bigquery",
                  dataset_name="{{ kv('DATASET_NAME') }}",
                  dev_mode=False,
              )

              load_info = pipeline.run(races)
              print(load_info)

triggers:
- id: schedule
  type: io.kestra.plugin.core.trigger.Schedule
  cron: "@weekly"